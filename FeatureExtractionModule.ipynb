{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712421c-f5b0-482e-8cdd-b0ac631d637a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############# GSR Proccessig and Feature Extraction ############## #Takes about 23 minutes#\n",
    "import pandas as pd\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Directory paths\n",
    "block_details_dir = '/Users/cahree/Desktop/REU-2024-Cognitive-Load/CLAS_Database/CLAS/Block_details/'\n",
    "block_gsr_ppg_data_path = '/Users/cahree/Desktop/REU-2024-Cognitive-Load/CLAS_Database/CLAS/Participants/'\n",
    "\n",
    "# Get all block details CSV files\n",
    "all_users_block_details_paths = glob.glob(os.path.join(block_details_dir, '*.csv'))\n",
    "\n",
    "# Blocks of interest\n",
    "blocks_of_interest = [1, 2, 4, 8, 10]\n",
    "\n",
    "# Cognitive load labels based on block numbers\n",
    "cognitive_load_labels = {1: 1, 4: 1, 10: 1, 2: 2, 8: 2}\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "count = 0\n",
    "\n",
    "# Iterate over each block details file\n",
    "for block_details_path in all_users_block_details_paths:\n",
    "    #if count == 3:\n",
    "        #break\n",
    "    print(count)\n",
    "    count+=1\n",
    "    block_details = pd.read_csv(block_details_path)\n",
    "    filtered_block_details = block_details[block_details['Block'].isin(blocks_of_interest)]\n",
    "    sorted_filtered_block_details = filtered_block_details.sort_values(by='Block')\n",
    "\n",
    "    user_id = os.path.basename(block_details_path).split('_')[0]\n",
    "\n",
    "    # Iterate through each row in block details to load corresponding PPG files\n",
    "    for _, row in sorted_filtered_block_details.iterrows():\n",
    "        block = row['Block']\n",
    "        ppg_file = row['EDA&PPG File']\n",
    "\n",
    "        # Determine the correct filename for special blocks\n",
    "        if block == 2:\n",
    "            type = 'mathtest'\n",
    "            base, ext = ppg_file.rsplit('.', 1)\n",
    "            new_filename = f\"{base}{type}.{ext}\"\n",
    "        elif block == 8:\n",
    "            type = 'IQtest'\n",
    "            base, ext = ppg_file.rsplit('.', 1)\n",
    "            new_filename = f\"{base}{type}.{ext}\"\n",
    "        else: \n",
    "            new_filename = ppg_file\n",
    "        \n",
    "        file_path = os.path.join(block_gsr_ppg_data_path, user_id, 'by_block', new_filename)\n",
    "        \n",
    "        try:\n",
    "            # Load the PPG and GSR data\n",
    "            block_data = pd.read_csv(file_path)\n",
    "            gsr_data = block_data['gsr']\n",
    "\n",
    "            # Process and clean GSR data\n",
    "            gsr_data_cleaned = nk.eda_clean(gsr_data, sampling_rate=256, method=\"BioSPPy\")\n",
    "            cvxEDA = nk.eda_phasic(gsr_data_cleaned, sampling_rate=256, method=\"cvxEDA\")\n",
    "\n",
    "            # Extract tonic and phasic components\n",
    "            t1 = cvxEDA[\"EDA_Tonic\"].values\n",
    "            p1 = cvxEDA[\"EDA_Phasic\"].values\n",
    "\n",
    "            # Calculate the mean EDA from the cleaned signal\n",
    "            mean_eda = gsr_data_cleaned.mean()\n",
    "    \n",
    "\n",
    "            # Extract EDA peaks\n",
    "            _, eda_features = nk.eda_peaks(p1, sampling_rate=256, method=\"neurokit\")\n",
    "            # print(eda_features.keys())\n",
    "\n",
    "            # Convert the extracted features to a DataFrame\n",
    "            eda_features_df = pd.DataFrame({\n",
    "                'SCR_Onsets': eda_features['SCR_Onsets'],\n",
    "                'SCR_Peaks': eda_features['SCR_Peaks'],\n",
    "                'SCR_Height': eda_features['SCR_Height'],\n",
    "                'SCR_Amplitude': eda_features['SCR_Amplitude'],\n",
    "                'SCR_RiseTime': eda_features['SCR_RiseTime'],\n",
    "                'SCR_Recovery': eda_features['SCR_Recovery'],\n",
    "                'SCR_RecoveryTime': eda_features['SCR_RecoveryTime'],\n",
    "                'mean_eda':  mean_eda\n",
    "            })\n",
    "\n",
    "            # Handle missing values by imputing\n",
    "            eda_features_df = eda_features_df.fillna(0)\n",
    "\n",
    "            # Append each feature set with its corresponding label\n",
    "            for _, feature_set in eda_features_df.iterrows():\n",
    "                features.append(feature_set)\n",
    "                labels.append(cognitive_load_labels[block])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing PPG data for User {user_id}, Block {block}: {e}\")\n",
    "\n",
    "print(len(features))\n",
    "print(len(labels))\n",
    "\n",
    "features_df = pd.DataFrame(features)\n",
    "display(features_df)\n",
    "labels_df = pd.DataFrame(labels, columns=['Cognitive_Load_Label'])\n",
    "\n",
    "save_dir = '/Users/cahree/Desktop/REU-2024-Cognitive-Load'\n",
    "\n",
    "# Convert features and labels lists to DataFrames\n",
    "features_df = pd.DataFrame(features)\n",
    "labels_df = pd.DataFrame(labels, columns=['Cognitive_Load_Label'])\n",
    "\n",
    "# Reset indices of features and labels DataFrames\n",
    "features_df = features_df.reset_index(drop=True)\n",
    "labels_df = labels_df.reset_index(drop=True)\n",
    "\n",
    "# Optionally, concatenate the features and labels into a single DataFrame\n",
    "combined_df = pd.concat([features_df, labels_df], axis=1)\n",
    "\n",
    "# Define full paths for each CSV file\n",
    "features_path = os.path.join(save_dir, 'GSRfeatures.csv')\n",
    "labels_path = os.path.join(save_dir, 'GSRlabels.csv')\n",
    "combined_path = os.path.join(save_dir, 'GSRfeatures_with_labels.csv')\n",
    "\n",
    "# Save the DataFrames to the specified paths\n",
    "features_df.to_csv(features_path, index=False)\n",
    "labels_df.to_csv(labels_path, index=False)\n",
    "combined_df.to_csv(combined_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b168b60-1a91-4eb7-8b1c-23a36fefee22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc252167-17be-4487-a9e1-1a60bb7d0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install SHAP\n",
    "#!pip install neurokit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7579f86-71b1-4bd3-aec9-43184b568d6f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### PPG Proccessig and Feature Extraction ##########  #Takes abut 3 min #\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Directory paths\n",
    "block_details_dir = '/Users/cahree/Desktop/REU-2024-Cognitive-Load/CLAS_Database/CLAS/Block_details/'\n",
    "block_gsr_ppg_data_path = '/Users/cahree/Desktop/REU-2024-Cognitive-Load/CLAS_Database/CLAS/Participants/'\n",
    "\n",
    "# Get all block details CSV files\n",
    "all_users_block_details_paths = glob.glob(os.path.join(block_details_dir, '*.csv'))\n",
    "\n",
    "# Blocks of interest\n",
    "blocks_of_interest = [1, 2, 4, 8, 10]\n",
    "\n",
    "# Cognitive load labels based on block numbers\n",
    "cognitive_load_labels = {1: 1, 4: 1, 10: 1, 2: 2, 8: 2}\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "count = 0\n",
    "\n",
    "# Iterate over each block details file\n",
    "for block_details_path in all_users_block_details_paths:\n",
    "    #if count == 3:\n",
    "       # break\n",
    "    count += 1\n",
    "    print(count)\n",
    "    block_details = pd.read_csv(block_details_path)\n",
    "    filtered_block_details = block_details[block_details['Block'].isin(blocks_of_interest)]\n",
    "    sorted_filtered_block_details = filtered_block_details.sort_values(by='Block')\n",
    "\n",
    "    user_id = os.path.basename(block_details_path).split('_')[0]\n",
    "\n",
    "    # Iterate through each row in block details to load corresponding PPG files\n",
    "    for _, row in sorted_filtered_block_details.iterrows():\n",
    "        block = row['Block']\n",
    "        ppg_file = row['EDA&PPG File']\n",
    "\n",
    "        if block == 2:\n",
    "            type = 'mathtest'\n",
    "            base, ext = ppg_file.rsplit('.', 1)\n",
    "            new_filename = f\"{base}{type}.{ext}\"\n",
    "            file_path = os.path.join(block_gsr_ppg_data_path, user_id, 'by_block', new_filename)\n",
    "            \n",
    "        elif block == 8:\n",
    "            type = 'IQtest'\n",
    "            base, ext = ppg_file.rsplit('.', 1)\n",
    "            new_filename = f\"{base}{type}.{ext}\"\n",
    "            file_path = os.path.join(block_gsr_ppg_data_path, user_id, 'by_block', new_filename)\n",
    "            \n",
    "        else: \n",
    "            file_path = os.path.join(block_gsr_ppg_data_path, user_id, 'by_block', ppg_file)\n",
    "\n",
    "        print(f\"Loading PPG data for User {user_id}, Block {block} from: {file_path}\")\n",
    "        \n",
    "        # Load the PPG and GSR data\n",
    "        try:\n",
    "            block_data = pd.read_csv(file_path)\n",
    "            # Extract PPG and GSR columns\n",
    "            ppg_data = block_data['ppg']\n",
    "\n",
    "            # Process the PPG data\n",
    "            ppg_signals, info = nk.ppg_process(ppg_data.values.flatten(), sampling_rate=256)\n",
    "            \n",
    "            # Analyze the PPG data using interval-related analysis\n",
    "            ppg_analysis = nk.ppg_analyze(ppg_signals, sampling_rate=256, method='interval-related')\n",
    "\n",
    "            # Extract features of interest\n",
    "            ppg_features_df = pd.DataFrame({\n",
    "                'HRV_RMSSD': ppg_analysis['HRV_RMSSD'],\n",
    "                'HRV_MeanNN': ppg_analysis['HRV_MeanNN'],\n",
    "                'HRV_SDNN': ppg_analysis['HRV_SDNN'],\n",
    "                'HRV_ApEn': ppg_analysis['HRV_ApEn'],\n",
    "                'PPG_Clean_Mean': ppg_signals['PPG_Clean'].mean()\n",
    "            })\n",
    "\n",
    "\n",
    "            # Append each feature set with its corresponding label\n",
    "            for _, feature_set in ppg_features_df.iterrows():\n",
    "                features.append(feature_set)\n",
    "                labels.append(cognitive_load_labels[block])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing PPG data for User {user_id}, Block {block}: {e}\")\n",
    "\n",
    "\n",
    "print(len(features))\n",
    "print(len(labels))\n",
    "\n",
    "features_df = pd.DataFrame(features)\n",
    "display(features_df)\n",
    "labels_df = pd.DataFrame(labels, columns=['Cognitive_Load_Label'])\n",
    "\n",
    "save_dir = '/Users/cahree/Desktop/REU-2024-Cognitive-Load'\n",
    "\n",
    "# Convert features and labels lists to DataFrames\n",
    "features_df = pd.DataFrame(features)\n",
    "labels_df = pd.DataFrame(labels, columns=['Cognitive_Load_Label'])\n",
    "\n",
    "# Reset indices of features and labels DataFrames\n",
    "features_df = features_df.reset_index(drop=True)\n",
    "labels_df = labels_df.reset_index(drop=True)\n",
    "\n",
    "combined_df = pd.concat([features_df, labels_df], axis=1)\n",
    "\n",
    "# paths for each CSV file\n",
    "features_path = os.path.join(save_dir, 'PPGfeatures.csv')\n",
    "labels_path = os.path.join(save_dir, 'PPGlabels.csv')\n",
    "combined_path = os.path.join(save_dir, 'PPGfeatures_with_labels.csv')\n",
    "\n",
    "# Save the DataFrames paths\n",
    "features_df.to_csv(features_path, index=False)\n",
    "labels_df.to_csv(labels_path, index=False)\n",
    "combined_df.to_csv(combined_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6129c769-7a3a-47b3-a091-06acd11433f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ad9af1fd6376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ad9af1fd6376>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;31m# Combine all feature matrices and labels into a single DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "################# EEG Proccessig and Feature Extraction ################### # Takes about 13 minutes #\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "#import EEGEXTRACT2 as eeg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "fs = 256\n",
    "\n",
    "def extract_user_id(file_name):\n",
    "    return file_name.split('/')[-1]\n",
    "    \n",
    "def extract_level_number(file_path):\n",
    "    # Extract the filename from the file path\n",
    "    filename = file_path.split('/')[-1]\n",
    "    \n",
    "    # Remove the .csv extension\n",
    "    base_name = filename.replace('.csv', '')\n",
    "    \n",
    "    # Find the last underscore and extract the number\n",
    "    number_str = base_name.split('_')[-1]\n",
    "    \n",
    "    try:\n",
    "        return int(number_str)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"No valid number found in filename\")\n",
    "\n",
    "def extract_all_levels_files(path):\n",
    "    all_scenario_files = glob(path + '/*data_level*')\n",
    "    all_scenario_files.sort(key=extract_user_id)\n",
    "    return all_scenario_files\n",
    "    \n",
    "def load_labels(user_id, label_path='/Users/cahree/Desktop/REU-2024-Cognitive-Load/CL_Drive_Database/Labels'):\n",
    "    \"\"\"Load the labels for a specific user based on user ID.\"\"\"\n",
    "    label_file = f\"{label_path}/{user_id}.csv\"  # Adjust the pattern based on actual file names\n",
    "    return pd.read_csv(label_file)\n",
    "    \n",
    "def extract_all_user_eeg_files(path):\n",
    "    all_user_eeg_files = glob(path + '/*')\n",
    "    return all_user_eeg_files\n",
    "\n",
    "def normalize_to_custom_range(data, new_min, new_max):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    return (data - min_val) / (max_val - min_val) * (new_max - new_min) + new_min\n",
    "\n",
    "\n",
    "def segment_data_func(file):\n",
    "    removed_segments = []\n",
    "    # print(\"running segment_data_func\")\n",
    "    \n",
    "    # Load the CSV data\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Check for NaNs in the entire file\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        print(f\"File {file} contains NaNs and will be skipped.\")\n",
    "        return None\n",
    "    \n",
    "    # Define the known sampling frequency\n",
    "    sfreq = 256  # Sampling frequency in Hz\n",
    "    \n",
    "    # Calculate the number of samples per 10-second segment\n",
    "    segment_length = int(10 * sfreq)\n",
    "    \n",
    "    # Check total length of data\n",
    "    total_samples = len(df)\n",
    "    \n",
    "    # Ensure we can create exactly 18 segments\n",
    "    if total_samples < 18 * segment_length:\n",
    "        print(f\"Warning: The data file {file} does not contain enough samples for 18 segments of 10 seconds each.\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize a list to store all segments\n",
    "    segmented_data = []\n",
    "    \n",
    "    # Segment the data into 10-second intervals\n",
    "    n_segments = 18\n",
    "    for i in range(n_segments):\n",
    "        start_sample = i * segment_length\n",
    "        end_sample = start_sample + segment_length\n",
    "        # Extract the segment\n",
    "        segment = df.iloc[start_sample:end_sample].copy()\n",
    "        # Add a segment identifier column\n",
    "        segment.loc[:, 'segment'] = i\n",
    "        # Append the segment to the list\n",
    "        segmented_data.append(segment)\n",
    "    \n",
    "    # Combine all segments into a single DataFrame\n",
    "    segmented_df = pd.concat(segmented_data, ignore_index=True)\n",
    "    \n",
    "    # Extract the channel columns (assuming channel columns are numeric and are not 'segment')\n",
    "    channels = [col for col in segmented_df.columns if col != 'segment' and col !='Timestamp']\n",
    "    # Determine the number of channels\n",
    "    n_channels = len(channels)\n",
    "    \n",
    "    # Prepare data for reshaping\n",
    "    reshaped_data = np.empty((n_channels, segment_length, len(segmented_data)))\n",
    "    for i, segment in enumerate(segmented_data):\n",
    "        for j, channel in enumerate(channels):\n",
    "            reshaped_data[j, :, i] = segment[channel].values\n",
    "            \n",
    "    # print(reshaped_data)\n",
    "    return reshaped_data, removed_segments\n",
    "\n",
    "def aggregate_connectivity_features(connectivity_features, epochs):\n",
    "    print(\"connectivity_features shape:\", connectivity_features.shape)\n",
    "    # Adjust according to the correct shape\n",
    "    # Assuming 6 pairs as you have (18, 6), modify aggregation logic as needed\n",
    "    num_channels = 4\n",
    "    num_pairs = num_channels * (num_channels - 1) // 2\n",
    "    try:\n",
    "        aggregated_features = np.mean(connectivity_features.reshape(-1, num_pairs, epochs), axis=1)\n",
    "    except ValueError as e:\n",
    "       #  print(\"Reshape failed:\", e)\n",
    "        # print(\"Connectivity features shape:\", connectivity_features.shape)\n",
    "        # print(\"Expected reshape dimensions:\", num_pairs, epochs)\n",
    "        raise\n",
    "    return aggregated_features\n",
    "\n",
    "\n",
    "\n",
    "def feature_extraction(eegData, bin_min, bin_max, binWidth):\n",
    "    n_channels = eegData.shape[0]\n",
    "    # print(\" n_channels: \",  n_channels)\n",
    "    epochs = eegData.shape[2]\n",
    "    features = []\n",
    "    feature_names = []\n",
    "\n",
    "    ShannonRes = eeg.shannonEntropy(eegData, bin_min, bin_max, binWidth)\n",
    "    ShannonRes = ShannonRes.T\n",
    "    features.append(ShannonRes)\n",
    "    # print(\"ShannonRes\", ShannonRes.shape)\n",
    "    feature_names.extend([f\"ShannonEntropy_{i}\" for i in range(ShannonRes.shape[1])])\n",
    "\n",
    "    medianFreqRes = eeg.medianFreq(eegData, fs)\n",
    "    medianFreqRes = medianFreqRes.T\n",
    "    features.append(medianFreqRes)\n",
    "    feature_names.extend([f\"MedianFreq_{i}\" for i in range(medianFreqRes.shape[1])])\n",
    "\n",
    "    std_res = eeg.eegStd(eegData)\n",
    "    std_res = std_res.T\n",
    "    features.append(std_res)\n",
    "    feature_names.extend([f\"Std_{i}\" for i in range(std_res.shape[1])])\n",
    "\n",
    "    subbands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "    for band in subbands:\n",
    "        freq_bands = {'delta': (0.5, 4), 'theta': (4, 8), 'alpha': (8, 12), 'beta': (12, 30), 'gamma': (30, 100)}\n",
    "        eegData_band = eeg.filt_data(eegData, *freq_bands[band], fs)\n",
    "        ShannonRes_band = eeg.shannonEntropy(eegData_band, bin_min, bin_max, binWidth)\n",
    "        ShannonRes_band = ShannonRes_band.T\n",
    "        features.append(ShannonRes_band)\n",
    "        feature_names.extend([f\"ShannonEntropy_{band}_{i}\" for i in range(ShannonRes_band.shape[1])])\n",
    "\n",
    "    HjorthMob, HjorthComp = eeg.hjorthParameters(eegData)\n",
    "    HjorthMob = HjorthMob.T\n",
    "    HjorthComp = HjorthComp.T\n",
    "    features.append(HjorthMob)\n",
    "    features.append(HjorthComp)\n",
    "    feature_names.extend([f\"HjorthMob_{i}\" for i in range(HjorthMob.shape[1])])\n",
    "    feature_names.extend([f\"HjorthComp_{i}\" for i in range(HjorthComp.shape[1])])\n",
    "\n",
    "    bands = ['alpha', 'beta', 'gamma']\n",
    "    for band in bands:\n",
    "        freq_bands = {'alpha': (8, 12), 'beta': (12, 30), 'gamma': (30, 100)}\n",
    "        bandPwr = eeg.bandPower(eegData, *freq_bands[band], fs)\n",
    "        bandPwr = bandPwr.T\n",
    "        features.append(bandPwr)\n",
    "        feature_names.extend([f\"BandPower_{band}_{i}\" for i in range(bandPwr.shape[1])])\n",
    "    mi_features = []\n",
    "    for ch1 in range(n_channels):\n",
    "        for ch2 in range(ch1 + 1, n_channels):\n",
    "            mutual_info = eeg.calculate2Chan_MI(eegData, ch1, ch2, bin_min, bin_max, binWidth)\n",
    "            if mutual_info.ndim == 1:\n",
    "                mutual_info = mutual_info[:, np.newaxis]\n",
    "            mi_features.append(mutual_info)\n",
    "    if mi_features:\n",
    "        mi_features = np.concatenate(mi_features, axis=1)\n",
    "        # print(\"mi_features shape before aggregation:\", mi_features.shape)\n",
    "        mi_features = aggregate_connectivity_features(mi_features, epochs)\n",
    "        mi_features = mi_features.T\n",
    "        features.append(mi_features)\n",
    "        feature_names.extend([f\"MI_{i}\" for i in range(mi_features.shape[1])])\n",
    "\n",
    "    pli_features = []\n",
    "    for ch1 in range(n_channels):\n",
    "        for ch2 in range(ch1 + 1, n_channels):\n",
    "            pli = eeg.phaseLagIndex(eegData, ch1, ch2)\n",
    "            if pli.ndim == 1:\n",
    "                pli = pli[:, np.newaxis]\n",
    "            pli_features.append(pli)\n",
    "    if pli_features:\n",
    "        pli_features = np.concatenate(pli_features, axis=1)\n",
    "        pli_features = aggregate_connectivity_features(pli_features, epochs)\n",
    "        pli_features = pli_features.T\n",
    "        features.append(pli_features)\n",
    "        feature_names.extend([f\"PLI_{i}\" for i in range(pli_features.shape[1])])\n",
    "\n",
    "    try:\n",
    "        concatenated_features = np.concatenate([f for f in features], axis=1)\n",
    "        print(\"Concatenated Features Shape:\", concatenated_features.shape)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in concatenating features: {e}\")\n",
    "        for i, f in enumerate(features):\n",
    "            print(f\"Feature {i} shape: {f.shape}\")\n",
    "        raise\n",
    "\n",
    "    return concatenated_features, feature_names\n",
    "\n",
    "\n",
    "    \n",
    "    # Assuming df_features and df_labels are already defined and populated in the main function\n",
    "    \n",
    "def train_and_evaluate_models(features, labels):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Define models\n",
    "    models = {\n",
    "        'SVM': SVC(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'MLP': MLPClassifier(),\n",
    "        'KNN': KNeighborsClassifier()\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        print(f\"--- {name} ---\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"-------------------\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    # print(\"running main\")\n",
    "    count = 1\n",
    "    count_1 = 0\n",
    "    track = 0\n",
    "    all_user_eeg_files = extract_all_user_eeg_files('/Users/cahree/Desktop/REU-2024-Cognitive-Load/CL_Drive_Database/EEG')\n",
    "    \n",
    "    for eeg_file in all_user_eeg_files:\n",
    "        print(eeg_file)\n",
    "        #if count_1 >= 10:\n",
    "            #break\n",
    "        count_1 += 1\n",
    "        print(\"user count:\", count_1)\n",
    "\n",
    "        user_id = extract_user_id(eeg_file)  # Extract user ID\n",
    "        labels_df = load_labels(user_id)  # Load labels for the user\n",
    "        # display(labels_df)\n",
    "        \n",
    "        all_stimuli_levels_eeg_data = extract_all_levels_files(eeg_file)\n",
    "        for level_file in all_stimuli_levels_eeg_data:\n",
    "            print(\"current_level file: \", level_file)\n",
    "            level = extract_level_number(level_file)\n",
    "            #print(\"count:\", count)\n",
    "            #if count == 10:\n",
    "               # count = 1\n",
    "                \n",
    "            result = segment_data_func(level_file)\n",
    "            if result is None:\n",
    "                continue\n",
    "                \n",
    "            level_labels = labels_df[f\"lvl_{level}\"]\n",
    "            \n",
    "            # Convert cognitive load ratings to 'low', 'medium', 'high'\n",
    "            labels_df[f\"lvl_{level}\"] = labels_df[f\"lvl_{level}\"].apply(lambda x: 'low' if 1 <= x <= 3 else ('medium' if 4 <= x <= 6 else 'high'))\n",
    "            level_labels = labels_df[f\"lvl_{level}\"].values\n",
    "\n",
    "            segmented_data, removed_segments = result\n",
    "            # print(\"removed_segments:\", removed_segments)\n",
    "            print(segmented_data.shape)\n",
    "            \n",
    "            # Normalize and extract features\n",
    "            data_min = np.min(segmented_data)\n",
    "            data_max = np.max(segmented_data)\n",
    "            binWidth = 1\n",
    "            bin_min = data_min\n",
    "            bin_max = data_max + binWidth\n",
    "            normalized_eeg_data = normalize_to_custom_range(segmented_data, bin_min, bin_max)\n",
    "            print(\"Normalized EEG Data Shape:\", normalized_eeg_data.shape)\n",
    "            features, feature_names = feature_extraction(normalized_eeg_data, bin_min, bin_max, binWidth)\n",
    "            print(\"Extracted Features Shape:\", features.shape)\n",
    "            \n",
    "            # Append features and labels to lists\n",
    "            features_list.append(features)\n",
    "            labels_list.append(level_labels)\n",
    "            count += 1\n",
    "            #labels_list.append(labels_df['label'].values[:features.shape[0]])  # Adjust label slicing as needed\n",
    "           \n",
    "    # Combine all feature matrices and labels into a single DataFrame\n",
    "    all_features = np.vstack(features_list)\n",
    "    all_labels = np.hstack(labels_list)\n",
    "    print(all_labels.shape)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    features_df = pd.DataFrame(all_features, columns=feature_names)\n",
    "    \n",
    "    labels_df = pd.DataFrame(all_labels, columns=['label'])\n",
    "    # df['label'] = all_labels\n",
    "    # display(df_features)\n",
    "    # display(df_labels)\n",
    " \n",
    "    # Convert labels to numeric format for classification\n",
    "    labels_df['label'] = labels_df['label'].map({'low': 0, 'medium': 1, 'high': 2})\n",
    "\n",
    "    save_dir = '/Users/cahree/Desktop/REU-2024-Cognitive-Load'\n",
    "        \n",
    "    # Reset indices of features and labels DataFrames\n",
    "    features_df = features_df.reset_index(drop=True)\n",
    "    labels_df = labels_df.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    combined_df = pd.concat([features_df, labels_df], axis=1)\n",
    "    \n",
    "    # paths for each CSV file\n",
    "    features_path = os.path.join(save_dir, 'EEGfeatures.csv')\n",
    "    labels_path = os.path.join(save_dir, 'EEGlabels.csv')\n",
    "    combined_path = os.path.join(save_dir, 'EEGfeatures_with_labels.csv')\n",
    "    \n",
    "    # Save the DataFrames paths\n",
    "    features_df.to_csv(features_path, index=False)\n",
    "    labels_df.to_csv(labels_path, index=False)\n",
    "    combined_df.to_csv(combined_path, index=False)\n",
    "    \n",
    "        # Define the path and filename\n",
    "    path = '/Users/cahree/CL-DRIVE-HELPER/'\n",
    "    \n",
    "        # Save DataFrame to CSV at the specified path\n",
    "    df_features.to_csv(path + 'EEGFEATURES3.csv', index=False)\n",
    "    \n",
    "    df_labels.to_csv(path + 'EEGLABELS3.csv', index=False)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    features_array = features_df.to_numpy()\n",
    "    labels_array = labels_df['label'].to_numpy()  # Use only the numeric labels\n",
    "    \n",
    "    # Call the function to train and evaluate models\n",
    "    train_and_evaluate_models(features_array, labels_array)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710e6bc-a885-4d34-bf28-6fff826bd7de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################## PPG + GSR Multimodal Classification ##################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Load Data\n",
    "ppg_features = pd.read_csv('/Users/cahree/PPG-Features/features.csv')  \n",
    "ppg_labels = pd.read_csv('/Users/cahree/PPG-Features/labels.csv')    \n",
    "\n",
    "gsr_features = pd.read_csv('/Users/cahree/GSR-Features/features.csv')  \n",
    "gsr_labels = pd.read_csv('/Users/cahree/GSR-Features/labels.csv')    \n",
    "\n",
    "\n",
    "ppg_samples = len(ppg_features)\n",
    "gsr_samples = len(gsr_features)\n",
    "\n",
    "\n",
    "# Downsample GSR data to match the PPG samples exactly\n",
    "def downsample_data(data, num_samples):\n",
    "    # Calculate the number of samples to include in each bin\n",
    "    factor = len(data) / num_samples\n",
    "    indices = np.arange(0, len(data), factor)\n",
    "    return data.iloc[np.floor(indices).astype(int)]\n",
    "\n",
    "# Downsample GSR features\n",
    "gsr_features_downsampled = downsample_data(gsr_features, ppg_samples)\n",
    "\n",
    "\n",
    "print(\"Original GSR Features shape:\", gsr_features.shape)\n",
    "print(\"Downsampled GSR Features shape:\", gsr_features_downsampled.shape)\n",
    "\n",
    "# Ensure the length matches the PPG features\n",
    "assert len(gsr_features_downsampled) == ppg_samples, #\"Downsampling did not match the number of PPG samples.\"\n",
    "\n",
    "\n",
    "combined_features = pd.concat([ppg_features, gsr_features_downsampled.reset_index(drop=True)], axis=1)\n",
    "combined_labels = ppg_labels.squeeze()  # Ensure it's a Series\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "combined_features_imputed = pd.DataFrame(imputer.fit_transform(combined_features), columns=combined_features.columns)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "combined_features_scaled = pd.DataFrame(scaler.fit_transform(combined_features_imputed), columns=combined_features.columns)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(combined_features_scaled, combined_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=1200),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Multi-Layer Perceptron': MLPClassifier(max_iter=500, random_state=1200),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=1200)\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    \n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{name} Classification Report:\")\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    print(f\"{name} Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1326b32-593b-4a72-84db-ded93e0408e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################### PPG FEATURE IMPORTANCE (SHAP) #####################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "######----------For Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15,5)})\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "######----------For Feature Selection and Modeling\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "######---------For SHAP/Model Explanations\n",
    "import shap\n",
    "\n",
    "# Load Data\n",
    "X = pd.read_csv('/Users/cahree/PPG-Features/features.csv')  # Replace with the actual path to your features CSV\n",
    "Y = pd.read_csv('/Users/cahree/PPG-Features/labels.csv')    # Replace with the actual path to your labels CSV\n",
    "\n",
    "# Ensure that Y is a Series\n",
    "Y = Y.squeeze()  # This is necessary if the labels are loaded as a DataFrame with a single column\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "clf0 = RandomForestClassifier(n_estimators=100, random_state=1200)\n",
    "clf0.fit(X_train, Y_train)\n",
    "\n",
    "# Compute standard deviation of feature importances\n",
    "std = np.std([tree.feature_importances_ for tree in clf0.estimators_], axis=0)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = pd.Series(clf0.feature_importances_, index=X_train.columns)\n",
    "\n",
    "# Print accuracy on test data\n",
    "print('Score of RF model on test split\\n', clf0.score(X_test, Y_test))\n",
    "\n",
    "# Plot feature importances\n",
    "fig, ax = plt.subplots()\n",
    "feature_importances.plot.bar(yerr=std, ax=ax, color='red')  # Change 'orange' to your desired color\n",
    "ax.set_title(\"PPG Feature Importances Random Forest Clf\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d4383-f4de-4ead-a0a5-27536cad6d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################## GSR FEATURE IMPORTANCE (SHAP) ###################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "######----------For Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15,5)})\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "######----------For Feature Selection and Modeling\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "######---------For SHAP/Model Explanations\n",
    "import shap\n",
    "\n",
    "# Load Data\n",
    "X = pd.read_csv('/Users/cahree/GSR-Features/features.csv')  \n",
    "Y = pd.read_csv('/Users/cahree/GSR-Features/labels.csv')    \n",
    "\n",
    "# Ensure that Y is a Series\n",
    "Y = Y.squeeze()  # This is necessary if the labels are loaded as a DataFrame with a single column\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "clf0 = RandomForestClassifier(n_estimators=100, random_state=1200)\n",
    "clf0.fit(X_train, Y_train)\n",
    "\n",
    "# Compute standard deviation of feature importances\n",
    "std = np.std([tree.feature_importances_ for tree in clf0.estimators_], axis=0)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = pd.Series(clf0.feature_importances_, index=X_train.columns)\n",
    "\n",
    "# Print accuracy on test data\n",
    "print('Score of RF model on test split\\n', clf0.score(X_test, Y_test))\n",
    "\n",
    "# Plot feature importances\n",
    "fig, ax = plt.subplots()\n",
    "feature_importances.plot.bar(yerr=std, ax=ax, color='green')  # Change 'orange' to your desired color\n",
    "ax.set_title(\"GSR Feature Importances Random Forest Clf\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cca74f-af77-42c2-9216-ff7c4bac2bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################ EEG FEATURE IMPORTANCE SHAP ########################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load data from CSV files\n",
    "X = pd.read_csv('/Users/cahree/CL-DRIVE-HELPER/EEGFEATURES3.csv')  \n",
    "Y = pd.read_csv('/Users/cahree/CL-DRIVE-HELPER/EEGLABELS3.csv')    \n",
    "\n",
    "# Ensure that Y is a Series\n",
    "Y = Y.squeeze()  # since labels are loaded as a DataFrame with a single column\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "clf0 = RandomForestClassifier(n_estimators=100, random_state=1200)\n",
    "clf0.fit(X_train, Y_train)\n",
    "\n",
    "# Compute standard deviation of feature importances\n",
    "std = np.std([tree.feature_importances_ for tree in clf0.estimators_], axis=0)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = pd.Series(clf0.feature_importances_, index=X_train.columns)\n",
    "\n",
    "# Print accuracy on test data\n",
    "print('Score of RF model on test split\\n', clf0.score(X_test, Y_test))\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,5)})\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import shap\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "feature_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances Random Forest Clf\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8289eb6-44ea-40c0-abd3-f1af46947809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################## EEG BENCHMARK RESULLTS ############################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data from CSV files\n",
    "X = pd.read_csv('/Users/cahree/CL-DRIVE-HELPER/EEGFEATURES3.csv')  \n",
    "Y = pd.read_csv('/Users/cahree/CL-DRIVE-HELPER/EEGLABELS3.csv')    \n",
    "\n",
    "# Ensure that Y is a Series\n",
    "Y = Y.squeeze()  # This is necessary if the labels are loaded as a DataFrame with a single column\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=1200),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=1200),\n",
    "    'Support Vector Machine': SVC(random_state=1200),\n",
    "    'Multi-Layer Perceptron': MLPClassifier(random_state=1200, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train each classifier, evaluate accuracy, and print classification report\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(f'Accuracy of {name} on test data: {accuracy:.2f}')\n",
    "    print(f'Classification report for {name}:\\n')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    print('-' * 60)  # Separator between models for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738c261-64df-4d7b-9280-43beb1c69f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### EEG modeling ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load data from CSV files\n",
    "X = pd.read_csv('/Users/cahree/CL-DRIVE-HELPER/EEGFEATURES3.csv')  \n",
    "Y = pd.read_csv('/Users/cahree/CL-DRIVE-HELPER/EEGLABELS3.csv')    \n",
    "\n",
    "# Ensure that Y is a Series\n",
    "Y = Y.squeeze()  # This is necessary if the labels are loaded as a DataFrame with a single column\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=1200),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=1200),\n",
    "    'Support Vector Machine': SVC(random_state=1200),\n",
    "    'Multi-Layer Perceptron': MLPClassifier(random_state=1200, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Initialize a list to hold classifier names and their accuracies\n",
    "results = []\n",
    "\n",
    "# Train each classifier, evaluate accuracy, and collect results\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    results.append([name, accuracy])\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_results = pd.DataFrame(results, columns=['Classifier', 'Accuracy'])\n",
    "\n",
    "# Print the table in a professional format using tabulate\n",
    "print(tabulate(df_results, headers='keys', tablefmt='github', showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14e8a1-675e-4fc0-84dc-4ca992716784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################## EEG Modeling Benchmark reuslts ##################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tabulate import tabulate\n",
    "\n",
    "################################## EEG Modeling ##################################\n",
    "\n",
    "# Load data from CSV files\n",
    "X_eeg = pd.read_csv('/Users/cahree/CL-DRIVE-HELPER/EEGFEATURES3.csv')\n",
    "Y_eeg = pd.read_csv('/Users/cahree/CL-DRIVE-HELPER/EEGLABELS3.csv')\n",
    "\n",
    "Y_eeg = Y_eeg.squeeze()\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train_eeg, X_test_eeg, Y_train_eeg, Y_test_eeg = train_test_split(X_eeg, Y_eeg, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=1200),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=1200),\n",
    "    'Support Vector Machine': SVC(random_state=1200),\n",
    "    'Multi-Layer Perceptron': MLPClassifier(random_state=1200, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Initialize a list to hold classifier names and their accuracies for EEG\n",
    "results_eeg = []\n",
    "\n",
    "# Train each classifier for EEG data\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_eeg, Y_train_eeg)\n",
    "    Y_pred_eeg = clf.predict(X_test_eeg)\n",
    "    accuracy_eeg = accuracy_score(Y_test_eeg, Y_pred_eeg)\n",
    "    results_eeg.append(accuracy_eeg)\n",
    "\n",
    "################################## PPG + GSR Modeling ##################################\n",
    "\n",
    "# Load Data\n",
    "ppg_features = pd.read_csv('/Users/cahree/PPG-Features/features.csv')  \n",
    "ppg_labels = pd.read_csv('/Users/cahree/PPG-Features/labels.csv')    \n",
    "\n",
    "gsr_features = pd.read_csv('/Users/cahree/GSR-Features/features.csv')  \n",
    "gsr_labels = pd.read_csv('/Users/cahree/GSR-Features/labels.csv')    \n",
    "\n",
    "\n",
    "ppg_samples = len(ppg_features)\n",
    "\n",
    "# Downsample GSR data to match the PPG samples exactly\n",
    "def downsample_data(data, num_samples):\n",
    "    factor = len(data) / num_samples\n",
    "    indices = np.arange(0, len(data), factor)\n",
    "    return data.iloc[np.floor(indices).astype(int)]\n",
    "\n",
    "gsr_features_downsampled = downsample_data(gsr_features, ppg_samples)\n",
    "\n",
    "# Combine the features\n",
    "combined_features = pd.concat([ppg_features, gsr_features_downsampled.reset_index(drop=True)], axis=1)\n",
    "combined_labels = ppg_labels.squeeze()\n",
    "\n",
    "# Handle missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "combined_features_imputed = pd.DataFrame(imputer.fit_transform(combined_features), columns=combined_features.columns)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "combined_features_scaled = pd.DataFrame(scaler.fit_transform(combined_features_imputed), columns=combined_features.columns)\n",
    "\n",
    "X_train_ppg_gsr, X_test_ppg_gsr, Y_train_ppg_gsr, Y_test_ppg_gsr = train_test_split(combined_features_scaled, combined_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a list to hold classifier names and their accuracies for PPG + GSR\n",
    "results_ppg_gsr = []\n",
    "\n",
    "# Train each classifier for PPG + GSR data\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_ppg_gsr, Y_train_ppg_gsr)\n",
    "    Y_pred_ppg_gsr = clf.predict(X_test_ppg_gsr)\n",
    "    accuracy_ppg_gsr = accuracy_score(Y_test_ppg_gsr, Y_pred_ppg_gsr)\n",
    "    results_ppg_gsr.append(accuracy_ppg_gsr)\n",
    "\n",
    "################################## Combine Results ##################################\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "df_combined_results = pd.DataFrame({\n",
    "    'Classifier': classifiers.keys(),\n",
    "    'EEG Accuracy': results_eeg,\n",
    "    'PPG + GSR Accuracy': results_ppg_gsr\n",
    "})\n",
    "\n",
    "# Print the combined results in a professional table format using tabulate\n",
    "print(\"\\n\")\n",
    "print(tabulate(df_combined_results, headers='keys', tablefmt='github', showindex=False))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d9cf9-b29c-406e-9c89-b0a76e2eb913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
