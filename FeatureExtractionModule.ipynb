{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712421c-f5b0-482e-8cdd-b0ac631d637a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############# GSR Proccessig and Feature Extraction ############## #Takes about 23 minutes#\n",
    "import pandas as pd\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Directory paths\n",
    "block_details_dir = '/Users/cahree/Desktop/REU-2024-Cognitive-Load/CLAS_Database/CLAS/Block_details/'\n",
    "block_gsr_ppg_data_path = '/Users/cahree/Desktop/REU-2024-Cognitive-Load/CLAS_Database/CLAS/Participants/'\n",
    "\n",
    "# Get all block details CSV files\n",
    "all_users_block_details_paths = glob.glob(os.path.join(block_details_dir, '*.csv'))\n",
    "\n",
    "# Blocks of interest\n",
    "blocks_of_interest = [1, 2, 4, 8, 10]\n",
    "\n",
    "# Cognitive load labels based on block numbers\n",
    "cognitive_load_labels = {1: 1, 4: 1, 10: 1, 2: 2, 8: 2}\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "count = 0\n",
    "\n",
    "# Iterate over each block details file\n",
    "for block_details_path in all_users_block_details_paths:\n",
    "    #if count == 3:\n",
    "        #break\n",
    "    print(count)\n",
    "    count+=1\n",
    "    block_details = pd.read_csv(block_details_path)\n",
    "    filtered_block_details = block_details[block_details['Block'].isin(blocks_of_interest)]\n",
    "    sorted_filtered_block_details = filtered_block_details.sort_values(by='Block')\n",
    "\n",
    "    user_id = os.path.basename(block_details_path).split('_')[0]\n",
    "\n",
    "    # Iterate through each row in block details to load corresponding PPG files\n",
    "    for _, row in sorted_filtered_block_details.iterrows():\n",
    "        block = row['Block']\n",
    "        ppg_file = row['EDA&PPG File']\n",
    "\n",
    "        # Determine the correct filename for special blocks\n",
    "        if block == 2:\n",
    "            type = 'mathtest'\n",
    "            base, ext = ppg_file.rsplit('.', 1)\n",
    "            new_filename = f\"{base}{type}.{ext}\"\n",
    "        elif block == 8:\n",
    "            type = 'IQtest'\n",
    "            base, ext = ppg_file.rsplit('.', 1)\n",
    "            new_filename = f\"{base}{type}.{ext}\"\n",
    "        else: \n",
    "            new_filename = ppg_file\n",
    "        \n",
    "        file_path = os.path.join(block_gsr_ppg_data_path, user_id, 'by_block', new_filename)\n",
    "        \n",
    "        try:\n",
    "            # Load the PPG and GSR data\n",
    "            block_data = pd.read_csv(file_path)\n",
    "            gsr_data = block_data['gsr']\n",
    "\n",
    "            # Process and clean GSR data\n",
    "            gsr_data_cleaned = nk.eda_clean(gsr_data, sampling_rate=256, method=\"BioSPPy\")\n",
    "            cvxEDA = nk.eda_phasic(gsr_data_cleaned, sampling_rate=256, method=\"cvxEDA\")\n",
    "\n",
    "            # Extract tonic and phasic components\n",
    "            t1 = cvxEDA[\"EDA_Tonic\"].values\n",
    "            p1 = cvxEDA[\"EDA_Phasic\"].values\n",
    "\n",
    "            # Calculate the mean EDA from the cleaned signal\n",
    "            mean_eda = gsr_data_cleaned.mean()\n",
    "    \n",
    "\n",
    "            # Extract EDA peaks\n",
    "            _, eda_features = nk.eda_peaks(p1, sampling_rate=256, method=\"neurokit\")\n",
    "            # print(eda_features.keys())\n",
    "\n",
    "            # Convert the extracted features to a DataFrame\n",
    "            eda_features_df = pd.DataFrame({\n",
    "                'SCR_Onsets': eda_features['SCR_Onsets'],\n",
    "                'SCR_Peaks': eda_features['SCR_Peaks'],\n",
    "                'SCR_Height': eda_features['SCR_Height'],\n",
    "                'SCR_Amplitude': eda_features['SCR_Amplitude'],\n",
    "                'SCR_RiseTime': eda_features['SCR_RiseTime'],\n",
    "                'SCR_Recovery': eda_features['SCR_Recovery'],\n",
    "                'SCR_RecoveryTime': eda_features['SCR_RecoveryTime'],\n",
    "                'mean_eda':  mean_eda\n",
    "            })\n",
    "\n",
    "            # Handle missing values by imputing\n",
    "            eda_features_df = eda_features_df.fillna(0)\n",
    "\n",
    "            # Append each feature set with its corresponding label\n",
    "            for _, feature_set in eda_features_df.iterrows():\n",
    "                features.append(feature_set)\n",
    "                labels.append(cognitive_load_labels[block])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing PPG data for User {user_id}, Block {block}: {e}\")\n",
    "\n",
    "print(len(features))\n",
    "print(len(labels))\n",
    "\n",
    "features_df = pd.DataFrame(features)\n",
    "display(features_df)\n",
    "labels_df = pd.DataFrame(labels, columns=['Cognitive_Load_Label'])\n",
    "\n",
    "save_dir = '/Users/cahree/Desktop/REU-2024-Cognitive-Load'\n",
    "\n",
    "# Convert features and labels lists to DataFrames\n",
    "features_df = pd.DataFrame(features)\n",
    "labels_df = pd.DataFrame(labels, columns=['Cognitive_Load_Label'])\n",
    "\n",
    "# Reset indices of features and labels DataFrames\n",
    "features_df = features_df.reset_index(drop=True)\n",
    "labels_df = labels_df.reset_index(drop=True)\n",
    "\n",
    "# Optionally, concatenate the features and labels into a single DataFrame\n",
    "combined_df = pd.concat([features_df, labels_df], axis=1)\n",
    "\n",
    "# Define full paths for each CSV file\n",
    "features_path = os.path.join(save_dir, 'GSRfeatures.csv')\n",
    "labels_path = os.path.join(save_dir, 'GSRlabels.csv')\n",
    "combined_path = os.path.join(save_dir, 'GSRfeatures_with_labels.csv')\n",
    "\n",
    "# Save the DataFrames to the specified paths\n",
    "features_df.to_csv(features_path, index=False)\n",
    "labels_df.to_csv(labels_path, index=False)\n",
    "combined_df.to_csv(combined_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7579f86-71b1-4bd3-aec9-43184b568d6f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### PPG Proccessig and Feature Extraction ##########  #Takes abut 3 min #\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Directory paths\n",
    "block_details_dir = '/Users/cahree/Desktop/REU-2024-Cognitive-Load/CLAS_Database/CLAS/Block_details/'\n",
    "block_gsr_ppg_data_path = '/Users/cahree/Desktop/REU-2024-Cognitive-Load/CLAS_Database/CLAS/Participants/'\n",
    "\n",
    "# Get all block details CSV files\n",
    "all_users_block_details_paths = glob.glob(os.path.join(block_details_dir, '*.csv'))\n",
    "\n",
    "# Blocks of interest\n",
    "blocks_of_interest = [1, 2, 4, 8, 10]\n",
    "\n",
    "# Cognitive load labels based on block numbers\n",
    "cognitive_load_labels = {1: 1, 4: 1, 10: 1, 2: 2, 8: 2}\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "count = 0\n",
    "\n",
    "# Iterate over each block details file\n",
    "for block_details_path in all_users_block_details_paths:\n",
    "    #if count == 3:\n",
    "       # break\n",
    "    count += 1\n",
    "    print(count)\n",
    "    block_details = pd.read_csv(block_details_path)\n",
    "    filtered_block_details = block_details[block_details['Block'].isin(blocks_of_interest)]\n",
    "    sorted_filtered_block_details = filtered_block_details.sort_values(by='Block')\n",
    "\n",
    "    user_id = os.path.basename(block_details_path).split('_')[0]\n",
    "\n",
    "    # Iterate through each row in block details to load corresponding PPG files\n",
    "    for _, row in sorted_filtered_block_details.iterrows():\n",
    "        block = row['Block']\n",
    "        ppg_file = row['EDA&PPG File']\n",
    "\n",
    "        if block == 2:\n",
    "            type = 'mathtest'\n",
    "            base, ext = ppg_file.rsplit('.', 1)\n",
    "            new_filename = f\"{base}{type}.{ext}\"\n",
    "            file_path = os.path.join(block_gsr_ppg_data_path, user_id, 'by_block', new_filename)\n",
    "            \n",
    "        elif block == 8:\n",
    "            type = 'IQtest'\n",
    "            base, ext = ppg_file.rsplit('.', 1)\n",
    "            new_filename = f\"{base}{type}.{ext}\"\n",
    "            file_path = os.path.join(block_gsr_ppg_data_path, user_id, 'by_block', new_filename)\n",
    "            \n",
    "        else: \n",
    "            file_path = os.path.join(block_gsr_ppg_data_path, user_id, 'by_block', ppg_file)\n",
    "\n",
    "        print(f\"Loading PPG data for User {user_id}, Block {block} from: {file_path}\")\n",
    "        \n",
    "        # Load the PPG and GSR data\n",
    "        try:\n",
    "            block_data = pd.read_csv(file_path)\n",
    "            # Extract PPG and GSR columns\n",
    "            ppg_data = block_data['ppg']\n",
    "\n",
    "            # Process the PPG data\n",
    "            ppg_signals, info = nk.ppg_process(ppg_data.values.flatten(), sampling_rate=256)\n",
    "            \n",
    "            # Analyze the PPG data using interval-related analysis\n",
    "            ppg_analysis = nk.ppg_analyze(ppg_signals, sampling_rate=256, method='interval-related')\n",
    "\n",
    "            # Extract features of interest\n",
    "            ppg_features_df = pd.DataFrame({\n",
    "                'HRV_RMSSD': ppg_analysis['HRV_RMSSD'],\n",
    "                'HRV_MeanNN': ppg_analysis['HRV_MeanNN'],\n",
    "                'HRV_SDNN': ppg_analysis['HRV_SDNN'],\n",
    "                'HRV_ApEn': ppg_analysis['HRV_ApEn'],\n",
    "                'PPG_Clean_Mean': ppg_signals['PPG_Clean'].mean()\n",
    "            })\n",
    "\n",
    "\n",
    "            # Append each feature set with its corresponding label\n",
    "            for _, feature_set in ppg_features_df.iterrows():\n",
    "                features.append(feature_set)\n",
    "                labels.append(cognitive_load_labels[block])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing PPG data for User {user_id}, Block {block}: {e}\")\n",
    "\n",
    "\n",
    "print(len(features))\n",
    "print(len(labels))\n",
    "\n",
    "features_df = pd.DataFrame(features)\n",
    "display(features_df)\n",
    "labels_df = pd.DataFrame(labels, columns=['Cognitive_Load_Label'])\n",
    "\n",
    "save_dir = '/Users/cahree/Desktop/REU-2024-Cognitive-Load'\n",
    "\n",
    "# Convert features and labels lists to DataFrames\n",
    "features_df = pd.DataFrame(features)\n",
    "labels_df = pd.DataFrame(labels, columns=['Cognitive_Load_Label'])\n",
    "\n",
    "# Reset indices of features and labels DataFrames\n",
    "features_df = features_df.reset_index(drop=True)\n",
    "labels_df = labels_df.reset_index(drop=True)\n",
    "\n",
    "combined_df = pd.concat([features_df, labels_df], axis=1)\n",
    "\n",
    "# paths for each CSV file\n",
    "features_path = os.path.join(save_dir, 'PPGfeatures.csv')\n",
    "labels_path = os.path.join(save_dir, 'PPGlabels.csv')\n",
    "combined_path = os.path.join(save_dir, 'PPGfeatures_with_labels.csv')\n",
    "\n",
    "# Save the DataFrames paths\n",
    "features_df.to_csv(features_path, index=False)\n",
    "labels_df.to_csv(labels_path, index=False)\n",
    "combined_df.to_csv(combined_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129c769-7a3a-47b3-a091-06acd11433f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################# EEG Proccessig and Feature Extraction ################### # Takes about 13 minutes #\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "#import EEGEXTRACT2 as eeg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "fs = 256\n",
    "\n",
    "def extract_user_id(file_name):\n",
    "    return file_name.split('/')[-1]\n",
    "    \n",
    "def extract_level_number(file_path):\n",
    "    # Extract the filename from the file path\n",
    "    filename = file_path.split('/')[-1]\n",
    "    \n",
    "    # Remove the .csv extension\n",
    "    base_name = filename.replace('.csv', '')\n",
    "    \n",
    "    # Find the last underscore and extract the number\n",
    "    number_str = base_name.split('_')[-1]\n",
    "    \n",
    "    try:\n",
    "        return int(number_str)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"No valid number found in filename\")\n",
    "\n",
    "def extract_all_levels_files(path):\n",
    "    all_scenario_files = glob(path + '/*data_level*')\n",
    "    all_scenario_files.sort(key=extract_user_id)\n",
    "    return all_scenario_files\n",
    "    \n",
    "def load_labels(user_id, label_path='/Users/cahree/Desktop/REU-2024-Cognitive-Load/CL_Drive_Database/Labels'):\n",
    "    \"\"\"Load the labels for a specific user based on user ID.\"\"\"\n",
    "    label_file = f\"{label_path}/{user_id}.csv\"  # Adjust the pattern based on actual file names\n",
    "    return pd.read_csv(label_file)\n",
    "    \n",
    "def extract_all_user_eeg_files(path):\n",
    "    all_user_eeg_files = glob(path + '/*')\n",
    "    return all_user_eeg_files\n",
    "\n",
    "def normalize_to_custom_range(data, new_min, new_max):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    return (data - min_val) / (max_val - min_val) * (new_max - new_min) + new_min\n",
    "\n",
    "\n",
    "def segment_data_func(file):\n",
    "    removed_segments = []\n",
    "    # print(\"running segment_data_func\")\n",
    "    \n",
    "    # Load the CSV data\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Check for NaNs in the entire file\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        print(f\"File {file} contains NaNs and will be skipped.\")\n",
    "        return None\n",
    "    \n",
    "    # Define the known sampling frequency\n",
    "    sfreq = 256  # Sampling frequency in Hz\n",
    "    \n",
    "    # Calculate the number of samples per 10-second segment\n",
    "    segment_length = int(10 * sfreq)\n",
    "    \n",
    "    # Check total length of data\n",
    "    total_samples = len(df)\n",
    "    \n",
    "    # Ensure we can create exactly 18 segments\n",
    "    if total_samples < 18 * segment_length:\n",
    "        print(f\"Warning: The data file {file} does not contain enough samples for 18 segments of 10 seconds each.\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize a list to store all segments\n",
    "    segmented_data = []\n",
    "    \n",
    "    # Segment the data into 10-second intervals\n",
    "    n_segments = 18\n",
    "    for i in range(n_segments):\n",
    "        start_sample = i * segment_length\n",
    "        end_sample = start_sample + segment_length\n",
    "        # Extract the segment\n",
    "        segment = df.iloc[start_sample:end_sample].copy()\n",
    "        # Add a segment identifier column\n",
    "        segment.loc[:, 'segment'] = i\n",
    "        # Append the segment to the list\n",
    "        segmented_data.append(segment)\n",
    "    \n",
    "    # Combine all segments into a single DataFrame\n",
    "    segmented_df = pd.concat(segmented_data, ignore_index=True)\n",
    "    \n",
    "    # Extract the channel columns (assuming channel columns are numeric and are not 'segment')\n",
    "    channels = [col for col in segmented_df.columns if col != 'segment' and col !='Timestamp']\n",
    "    # Determine the number of channels\n",
    "    n_channels = len(channels)\n",
    "    \n",
    "    # Prepare data for reshaping\n",
    "    reshaped_data = np.empty((n_channels, segment_length, len(segmented_data)))\n",
    "    for i, segment in enumerate(segmented_data):\n",
    "        for j, channel in enumerate(channels):\n",
    "            reshaped_data[j, :, i] = segment[channel].values\n",
    "            \n",
    "    # print(reshaped_data)\n",
    "    return reshaped_data, removed_segments\n",
    "\n",
    "def aggregate_connectivity_features(connectivity_features, epochs):\n",
    "    print(\"connectivity_features shape:\", connectivity_features.shape)\n",
    "    # Adjust according to the correct shape\n",
    "    # Assuming 6 pairs as you have (18, 6), modify aggregation logic as needed\n",
    "    num_channels = 4\n",
    "    num_pairs = num_channels * (num_channels - 1) // 2\n",
    "    try:\n",
    "        aggregated_features = np.mean(connectivity_features.reshape(-1, num_pairs, epochs), axis=1)\n",
    "    except ValueError as e:\n",
    "       #  print(\"Reshape failed:\", e)\n",
    "        # print(\"Connectivity features shape:\", connectivity_features.shape)\n",
    "        # print(\"Expected reshape dimensions:\", num_pairs, epochs)\n",
    "        raise\n",
    "    return aggregated_features\n",
    "\n",
    "\n",
    "\n",
    "def feature_extraction(eegData, bin_min, bin_max, binWidth):\n",
    "    n_channels = eegData.shape[0]\n",
    "    # print(\" n_channels: \",  n_channels)\n",
    "    epochs = eegData.shape[2]\n",
    "    features = []\n",
    "    feature_names = []\n",
    "\n",
    "    ShannonRes = eeg.shannonEntropy(eegData, bin_min, bin_max, binWidth)\n",
    "    ShannonRes = ShannonRes.T\n",
    "    features.append(ShannonRes)\n",
    "    # print(\"ShannonRes\", ShannonRes.shape)\n",
    "    feature_names.extend([f\"ShannonEntropy_{i}\" for i in range(ShannonRes.shape[1])])\n",
    "\n",
    "    medianFreqRes = eeg.medianFreq(eegData, fs)\n",
    "    medianFreqRes = medianFreqRes.T\n",
    "    features.append(medianFreqRes)\n",
    "    feature_names.extend([f\"MedianFreq_{i}\" for i in range(medianFreqRes.shape[1])])\n",
    "\n",
    "    std_res = eeg.eegStd(eegData)\n",
    "    std_res = std_res.T\n",
    "    features.append(std_res)\n",
    "    feature_names.extend([f\"Std_{i}\" for i in range(std_res.shape[1])])\n",
    "\n",
    "    subbands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "    for band in subbands:\n",
    "        freq_bands = {'delta': (0.5, 4), 'theta': (4, 8), 'alpha': (8, 12), 'beta': (12, 30), 'gamma': (30, 100)}\n",
    "        eegData_band = eeg.filt_data(eegData, *freq_bands[band], fs)\n",
    "        ShannonRes_band = eeg.shannonEntropy(eegData_band, bin_min, bin_max, binWidth)\n",
    "        ShannonRes_band = ShannonRes_band.T\n",
    "        features.append(ShannonRes_band)\n",
    "        feature_names.extend([f\"ShannonEntropy_{band}_{i}\" for i in range(ShannonRes_band.shape[1])])\n",
    "\n",
    "    HjorthMob, HjorthComp = eeg.hjorthParameters(eegData)\n",
    "    HjorthMob = HjorthMob.T\n",
    "    HjorthComp = HjorthComp.T\n",
    "    features.append(HjorthMob)\n",
    "    features.append(HjorthComp)\n",
    "    feature_names.extend([f\"HjorthMob_{i}\" for i in range(HjorthMob.shape[1])])\n",
    "    feature_names.extend([f\"HjorthComp_{i}\" for i in range(HjorthComp.shape[1])])\n",
    "\n",
    "    bands = ['alpha', 'beta', 'gamma']\n",
    "    for band in bands:\n",
    "        freq_bands = {'alpha': (8, 12), 'beta': (12, 30), 'gamma': (30, 100)}\n",
    "        bandPwr = eeg.bandPower(eegData, *freq_bands[band], fs)\n",
    "        bandPwr = bandPwr.T\n",
    "        features.append(bandPwr)\n",
    "        feature_names.extend([f\"BandPower_{band}_{i}\" for i in range(bandPwr.shape[1])])\n",
    "    mi_features = []\n",
    "    for ch1 in range(n_channels):\n",
    "        for ch2 in range(ch1 + 1, n_channels):\n",
    "            mutual_info = eeg.calculate2Chan_MI(eegData, ch1, ch2, bin_min, bin_max, binWidth)\n",
    "            if mutual_info.ndim == 1:\n",
    "                mutual_info = mutual_info[:, np.newaxis]\n",
    "            mi_features.append(mutual_info)\n",
    "    if mi_features:\n",
    "        mi_features = np.concatenate(mi_features, axis=1)\n",
    "        # print(\"mi_features shape before aggregation:\", mi_features.shape)\n",
    "        mi_features = aggregate_connectivity_features(mi_features, epochs)\n",
    "        mi_features = mi_features.T\n",
    "        features.append(mi_features)\n",
    "        feature_names.extend([f\"MI_{i}\" for i in range(mi_features.shape[1])])\n",
    "\n",
    "    pli_features = []\n",
    "    for ch1 in range(n_channels):\n",
    "        for ch2 in range(ch1 + 1, n_channels):\n",
    "            pli = eeg.phaseLagIndex(eegData, ch1, ch2)\n",
    "            if pli.ndim == 1:\n",
    "                pli = pli[:, np.newaxis]\n",
    "            pli_features.append(pli)\n",
    "    if pli_features:\n",
    "        pli_features = np.concatenate(pli_features, axis=1)\n",
    "        pli_features = aggregate_connectivity_features(pli_features, epochs)\n",
    "        pli_features = pli_features.T\n",
    "        features.append(pli_features)\n",
    "        feature_names.extend([f\"PLI_{i}\" for i in range(pli_features.shape[1])])\n",
    "\n",
    "    try:\n",
    "        concatenated_features = np.concatenate([f for f in features], axis=1)\n",
    "        print(\"Concatenated Features Shape:\", concatenated_features.shape)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in concatenating features: {e}\")\n",
    "        for i, f in enumerate(features):\n",
    "            print(f\"Feature {i} shape: {f.shape}\")\n",
    "        raise\n",
    "\n",
    "    return concatenated_features, feature_names\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d9cf9-b29c-406e-9c89-b0a76e2eb913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
